% Encoding: UTF-8

@WWW{Senchanka2019,
  author  = {Senchanka, Pavel},
  date    = {2019-12-12},
  title   = {Example on-device model personalization with TensorFlow Lite},
  url     = {https://blog.tensorflow.org/2019/12/example-on-device-model-personalization.html},
  urldate = {2021-05-12},
}

@WWW{Sarkar2019,
  author  = {Dipanjan Sarkar},
  date    = {2019-11-14},
  title   = {A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning},
  url     = {https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a},
  urldate = {2021-05-12},
}

@Article{Pellegrini2019,
  author      = {Lorenzo Pellegrini and Gabriele Graffieti and Vincenzo Lomonaco and Davide Maltoni},
  date        = {2019-12-02},
  title       = {Latent Replay for Real-Time Continual Learning},
  eprint      = {1912.01100},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Training deep neural networks at the edge on light computational devices, embedded systems and robotic platforms is nowadays very challenging. Continual learning techniques, where complex models are incrementally trained on small batches of new data, can make the learning problem tractable even for CPU-only embedded devices enabling remarkable levels of adaptiveness and autonomy. However, a number of practical problems need to be solved: catastrophic forgetting before anything else. In this paper we introduce an original technique named "Latent Replay" where, instead of storing a portion of past data in the input space, we store activations volumes at some intermediate layer. This can significantly reduce the computation and storage required by native rehearsal. To keep the representation stable and the stored activations valid we propose to slow-down learning at all the layers below the latent replay one, leaving the layers above free to learn at full pace. In our experiments we show that Latent Replay, combined with existing continual learning techniques, achieves state-of-the-art performance on complex video benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d. batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly real-time continual learning on the edge through the deployment of the proposed technique on a smartphone device.},
  keywords    = {cs.LG, cs.CV, stat.ML},
}

@Article{forgetting,
  author      = {James Kirkpatrick and Razvan Pascanu and Neil Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and Agnieszka Grabska-Barwinska and Demis Hassabis and Claudia Clopath and Dharshan Kumaran and Raia Hadsell},
  title       = {Overcoming catastrophic forgetting in neural networks},
  date        = {2016-12-02},
  eprint      = {1612.00796v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
  file        = {online:http\://arxiv.org/pdf/1612.00796v2:PDF},
  keywords    = {cs.LG, cs.AI, stat.ML},
}

@Comment{jabref-meta: databaseType:biblatex;}
